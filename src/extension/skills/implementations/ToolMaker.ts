import * as vscode from 'vscode';
import * as path from 'path';
import * as fs from 'fs';
import { BaseSkill } from '../BaseSkill';
import { ProjectContext, SkillParams, SkillResult, SkillProgressReporter, SupportedLanguage } from '../interfaces';

/**
 * å·¥å…·ç±»å‹
 */
type ToolType = 
  | 'batch_rename'      // æ‰¹é‡é‡å‘½å
  | 'log_analyzer'      // æ—¥å¿—åˆ†æ
  | 'image_processor'   // å›¾ç‰‡å¤„ç†
  | 'file_organizer'    // æ–‡ä»¶æ•´ç†
  | 'data_converter'    // æ•°æ®è½¬æ¢
  | 'backup_tool'       // å¤‡ä»½å·¥å…·
  | 'text_processor'    // æ–‡æœ¬å¤„ç†
  | 'api_tester'        // APIæµ‹è¯•
  | 'port_scanner'      // ç«¯å£æ‰«æ
  | 'system_monitor'    // ç³»ç»Ÿç›‘æ§
  | 'custom';           // è‡ªå®šä¹‰å·¥å…·

/**
 * å·¥å…·é…ç½®
 */
interface ToolConfig {
  name: string;
  description: string;
  language: 'python' | 'bash' | 'node';
  template: string;
  dependencies?: string[];
}

/**
 * å°å·¥å…·åˆ¶ä½œæŠ€èƒ½
 * åˆ¶ä½œæœ¬åœ°CLI/å°è„šæœ¬å¹¶ç›´æ¥åœ¨ç»ˆç«¯è¿è¡Œ
 */
export class ToolMakerSkill extends BaseSkill {
  readonly id = 'tool-maker';
  readonly name = 'å°å·¥å…·åˆ¶ä½œå™¨';
  readonly description = 'åˆ¶ä½œæœ¬åœ°CLI/å°è„šæœ¬(æ‰¹é‡é‡å‘½åã€æ—¥å¿—åˆ†æã€å›¾ç‰‡å‹ç¼©ç­‰)å¹¶ç›´æ¥åœ¨ç»ˆç«¯è¿è¡Œ';
  readonly category = 'builder' as const;

  private toolTemplates: Record<ToolType, ToolConfig> = {
    batch_rename: {
      name: 'æ‰¹é‡é‡å‘½åå·¥å…·',
      description: 'æ‰¹é‡é‡å‘½åæ–‡ä»¶,æ”¯æŒæ­£åˆ™æ›¿æ¢ã€åºå·æ·»åŠ ã€æ—¥æœŸæ·»åŠ ç­‰',
      language: 'python',
      dependencies: [],
      template: this.getBatchRenameTemplate(),
    },
    log_analyzer: {
      name: 'æ—¥å¿—åˆ†æå·¥å…·',
      description: 'åˆ†ææ—¥å¿—æ–‡ä»¶,æå–é”™è¯¯ã€ç»Ÿè®¡é¢‘ç‡ã€ç”ŸæˆæŠ¥å‘Š',
      language: 'python',
      dependencies: [],
      template: this.getLogAnalyzerTemplate(),
    },
    image_processor: {
      name: 'å›¾ç‰‡æ‰¹å¤„ç†å·¥å…·',
      description: 'æ‰¹é‡å‹ç¼©ã€è°ƒæ•´å°ºå¯¸ã€æ·»åŠ æ°´å°ã€æ ¼å¼è½¬æ¢',
      language: 'python',
      dependencies: ['Pillow'],
      template: this.getImageProcessorTemplate(),
    },
    file_organizer: {
      name: 'æ–‡ä»¶æ•´ç†å·¥å…·',
      description: 'æŒ‰ç±»å‹/æ—¥æœŸ/å¤§å°è‡ªåŠ¨æ•´ç†æ–‡ä»¶',
      language: 'python',
      dependencies: [],
      template: this.getFileOrganizerTemplate(),
    },
    data_converter: {
      name: 'æ•°æ®æ ¼å¼è½¬æ¢å·¥å…·',
      description: 'JSON/CSV/XML/YAMLäº’è½¬',
      language: 'python',
      dependencies: ['PyYAML'],
      template: this.getDataConverterTemplate(),
    },
    backup_tool: {
      name: 'æ–‡ä»¶å¤‡ä»½å·¥å…·',
      description: 'å®šæ—¶å¤‡ä»½ã€å¢é‡å¤‡ä»½ã€å‹ç¼©å½’æ¡£',
      language: 'python',
      dependencies: [],
      template: this.getBackupToolTemplate(),
    },
    text_processor: {
      name: 'æ–‡æœ¬å¤„ç†å·¥å…·',
      description: 'æ‰¹é‡æŸ¥æ‰¾æ›¿æ¢ã€ç¼–ç è½¬æ¢ã€æ ¼å¼åŒ–',
      language: 'python',
      dependencies: ['chardet'],
      template: this.getTextProcessorTemplate(),
    },
    api_tester: {
      name: 'APIæµ‹è¯•å·¥å…·',
      description: 'æ‰¹é‡æµ‹è¯•APIæ¥å£ã€ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š',
      language: 'python',
      dependencies: ['requests'],
      template: this.getAPITesterTemplate(),
    },
    port_scanner: {
      name: 'ç«¯å£æ‰«æå·¥å…·',
      description: 'æ‰«æä¸»æœºå¼€æ”¾ç«¯å£',
      language: 'python',
      dependencies: [],
      template: this.getPortScannerTemplate(),
    },
    system_monitor: {
      name: 'ç³»ç»Ÿç›‘æ§å·¥å…·',
      description: 'ç›‘æ§CPUã€å†…å­˜ã€ç£ç›˜ä½¿ç”¨æƒ…å†µ',
      language: 'python',
      dependencies: ['psutil'],
      template: this.getSystemMonitorTemplate(),
    },
    custom: {
      name: 'è‡ªå®šä¹‰å·¥å…·',
      description: 'æ ¹æ®éœ€æ±‚è‡ªå®šä¹‰è„šæœ¬',
      language: 'python',
      dependencies: [],
      template: this.getCustomTemplate(),
    },
  };

  canExecute(_context: ProjectContext): boolean {
    return true;
  }

  async execute(context: ProjectContext, params: SkillParams, reporter: SkillProgressReporter): Promise<SkillResult> {
    reporter.report('å¼€å§‹åˆ›å»ºå°å·¥å…·...', 0);

    const userInput = params.userInput?.toLowerCase() || '';

    try {
      // è§£æå·¥å…·ç±»å‹
      reporter.startSubTask('åˆ†æå·¥å…·éœ€æ±‚');
      const toolType = this.parseToolType(userInput);
      const toolConfig = this.toolTemplates[toolType];
      reporter.completeSubTask('åˆ†æå·¥å…·éœ€æ±‚', true);
      reporter.report(`è¯†åˆ«ä¸º: ${toolConfig.name}`, 20);

      // æ ¹æ®ç”¨æˆ·è¾“å…¥å®šåˆ¶å·¥å…·
      reporter.startSubTask('å®šåˆ¶å·¥å…·è„šæœ¬');
      const customizedScript = this.customizeScript(toolConfig, userInput);
      reporter.completeSubTask('å®šåˆ¶å·¥å…·è„šæœ¬', true);
      reporter.report('è„šæœ¬å®šåˆ¶å®Œæˆ', 50);

      // ä¿å­˜è„šæœ¬
      reporter.startSubTask('ä¿å­˜å·¥å…·æ–‡ä»¶');
      const scriptPath = await this.saveScript(context.root, customizedScript, toolType, toolConfig.language);
      reporter.completeSubTask('ä¿å­˜å·¥å…·æ–‡ä»¶', true);

      // ç”Ÿæˆè¿è¡Œè¯´æ˜
      const runInstructions = this.generateRunInstructions(toolConfig, scriptPath);

      reporter.report('å°å·¥å…·åˆ›å»ºå®Œæˆ', 100);

      return {
        success: true,
        message: `âœ… ${toolConfig.name}å·²åˆ›å»º`,
        generatedFiles: [scriptPath],
        data: {
          toolType,
          toolConfig,
          scriptPath,
          runInstructions,
          dependencies: toolConfig.dependencies,
        },
      };
    } catch (error) {
      return {
        success: false,
        message: `å·¥å…·åˆ›å»ºå¤±è´¥: ${error instanceof Error ? error.message : String(error)}`,
      };
    }
  }

  private parseToolType(input: string): ToolType {
    if (input.includes('é‡å‘½å') || input.includes('rename')) {
      return 'batch_rename';
    }
    if (input.includes('æ—¥å¿—') || input.includes('log')) {
      return 'log_analyzer';
    }
    if (input.includes('å›¾ç‰‡') || input.includes('å›¾åƒ') || input.includes('image') || input.includes('å‹ç¼©')) {
      return 'image_processor';
    }
    if (input.includes('æ•´ç†') || input.includes('åˆ†ç±»') || input.includes('organize')) {
      return 'file_organizer';
    }
    if (input.includes('è½¬æ¢') || input.includes('convert') || input.includes('json') || input.includes('csv')) {
      return 'data_converter';
    }
    if (input.includes('å¤‡ä»½') || input.includes('backup')) {
      return 'backup_tool';
    }
    if (input.includes('æ–‡æœ¬') || input.includes('text') || input.includes('æ›¿æ¢')) {
      return 'text_processor';
    }
    if (input.includes('api') || input.includes('æ¥å£') || input.includes('æµ‹è¯•')) {
      return 'api_tester';
    }
    if (input.includes('ç«¯å£') || input.includes('æ‰«æ') || input.includes('port')) {
      return 'port_scanner';
    }
    if (input.includes('ç›‘æ§') || input.includes('monitor') || input.includes('cpu') || input.includes('å†…å­˜')) {
      return 'system_monitor';
    }
    return 'custom';
  }

  private customizeScript(config: ToolConfig, userInput: string): string {
    let script = config.template;
    
    // æ·»åŠ ç”¨æˆ·éœ€æ±‚æ³¨é‡Š
    script = script.replace(
      '# ç”¨æˆ·éœ€æ±‚: ',
      `# ç”¨æˆ·éœ€æ±‚: ${userInput || 'é€šç”¨å·¥å…·'}`
    );
    
    // æ·»åŠ ç”Ÿæˆæ—¶é—´
    script = script.replace(
      '# ç”Ÿæˆæ—¶é—´: ',
      `# ç”Ÿæˆæ—¶é—´: ${new Date().toISOString()}`
    );
    
    return script;
  }

  private async saveScript(root: string, content: string, toolType: string, language: string): Promise<string> {
    const toolsDir = path.join(root, '.ai-tools');
    
    if (!fs.existsSync(toolsDir)) {
      fs.mkdirSync(toolsDir, { recursive: true });
    }

    const ext = language === 'bash' ? 'sh' : language === 'node' ? 'js' : 'py';
    const timestamp = new Date().toISOString().replace(/[:.]/g, '-').slice(0, 19);
    const fileName = `${toolType}_${timestamp}.${ext}`;
    const filePath = path.join(toolsDir, fileName);

    fs.writeFileSync(filePath, content, 'utf-8');
    
    // è®¾ç½®å¯æ‰§è¡Œæƒé™
    if (process.platform !== 'win32') {
      fs.chmodSync(filePath, '755');
    }
    
    return filePath;
  }

  private generateRunInstructions(config: ToolConfig, scriptPath: string): string {
    let instructions = `
## ğŸ”§ ${config.name}

### å·¥å…·è¯´æ˜
${config.description}

### è¿è¡Œæ–¹å¼
`;

    if (config.dependencies && config.dependencies.length > 0) {
      instructions += `
\`\`\`bash
# 1. å®‰è£…ä¾èµ–
pip install ${config.dependencies.join(' ')}

# 2. è¿è¡Œè„šæœ¬
`;
    } else {
      instructions += `
\`\`\`bash
# è¿è¡Œè„šæœ¬
`;
    }

    if (config.language === 'python') {
      instructions += `python "${scriptPath}"`;
    } else if (config.language === 'bash') {
      instructions += `bash "${scriptPath}"`;
    } else {
      instructions += `node "${scriptPath}"`;
    }

    instructions += `
\`\`\`

### é…ç½®è¯´æ˜
æ‰“å¼€è„šæœ¬æ–‡ä»¶ï¼Œä¿®æ”¹é…ç½®åŒºåŸŸçš„å‚æ•°ä»¥é€‚åº”æ‚¨çš„éœ€æ±‚ã€‚

### æ³¨æ„äº‹é¡¹
1. é¦–æ¬¡è¿è¡Œå‰è¯·æ£€æŸ¥å¹¶ä¿®æ”¹é…ç½®
2. å»ºè®®å…ˆåœ¨æµ‹è¯•ç›®å½•è¿è¡Œ
3. é‡è¦æ–‡ä»¶è¯·å…ˆå¤‡ä»½
`;

    return instructions;
  }

  // ==================== å·¥å…·æ¨¡æ¿ ====================

  private getBatchRenameTemplate(): string {
    return `#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ‰¹é‡é‡å‘½åå·¥å…·
# ç”¨æˆ·éœ€æ±‚: 
# ç”Ÿæˆæ—¶é—´: 
"""

import os
import re
from datetime import datetime
import argparse

# ==================== é…ç½®åŒºåŸŸ ====================
CONFIG = {
    'target_dir': '.',           # ç›®æ ‡ç›®å½•
    'pattern': '*',              # æ–‡ä»¶åŒ¹é…æ¨¡å¼
    'dry_run': True,             # é¢„è§ˆæ¨¡å¼ï¼ˆä¸å®é™…é‡å‘½åï¼‰
    'recursive': False,          # æ˜¯å¦é€’å½’å­ç›®å½•
}

# é‡å‘½åè§„åˆ™
RENAME_RULES = {
    'mode': 'replace',           # æ¨¡å¼: replace/prefix/suffix/sequence/date
    'search': '',                # æŸ¥æ‰¾å†…å®¹ï¼ˆæ­£åˆ™è¡¨è¾¾å¼ï¼‰
    'replace': '',               # æ›¿æ¢å†…å®¹
    'prefix': '',                # å‰ç¼€
    'suffix': '',                # åç¼€
    'sequence_start': 1,         # åºå·èµ·å§‹å€¼
    'sequence_padding': 3,       # åºå·å¡«å……ä½æ•°
    'date_format': '%Y%m%d',     # æ—¥æœŸæ ¼å¼
}

# ==================== é‡å‘½åå‡½æ•° ====================
def get_new_name(filename, index, rules):
    """æ ¹æ®è§„åˆ™ç”Ÿæˆæ–°æ–‡ä»¶å"""
    name, ext = os.path.splitext(filename)
    
    if rules['mode'] == 'replace' and rules['search']:
        name = re.sub(rules['search'], rules['replace'], name)
    elif rules['mode'] == 'prefix':
        name = rules['prefix'] + name
    elif rules['mode'] == 'suffix':
        name = name + rules['suffix']
    elif rules['mode'] == 'sequence':
        seq = str(index + rules['sequence_start']).zfill(rules['sequence_padding'])
        name = f"{seq}_{name}"
    elif rules['mode'] == 'date':
        date_str = datetime.now().strftime(rules['date_format'])
        name = f"{date_str}_{name}"
    
    return name + ext

def batch_rename(config, rules):
    """æ‰¹é‡é‡å‘½å"""
    target_dir = config['target_dir']
    
    if not os.path.isdir(target_dir):
        print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {target_dir}")
        return
    
    files = []
    if config['recursive']:
        for root, _, filenames in os.walk(target_dir):
            for f in filenames:
                files.append(os.path.join(root, f))
    else:
        files = [os.path.join(target_dir, f) for f in os.listdir(target_dir) 
                 if os.path.isfile(os.path.join(target_dir, f))]
    
    print(f"æ‰¾åˆ° {len(files)} ä¸ªæ–‡ä»¶")
    print("-" * 50)
    
    renamed_count = 0
    for i, filepath in enumerate(sorted(files)):
        dirname = os.path.dirname(filepath)
        filename = os.path.basename(filepath)
        new_name = get_new_name(filename, i, rules)
        new_path = os.path.join(dirname, new_name)
        
        if filename != new_name:
            print(f"  {filename}")
            print(f"    -> {new_name}")
            
            if not config['dry_run']:
                try:
                    os.rename(filepath, new_path)
                    renamed_count += 1
                except Exception as e:
                    print(f"    âŒ å¤±è´¥: {e}")
    
    print("-" * 50)
    if config['dry_run']:
        print("âš ï¸  é¢„è§ˆæ¨¡å¼ï¼Œæœªå®é™…é‡å‘½å")
        print("   è®¾ç½® dry_run = False ä»¥æ‰§è¡Œé‡å‘½å")
    else:
        print(f"âœ… å·²é‡å‘½å {renamed_count} ä¸ªæ–‡ä»¶")

# ==================== ä¸»ç¨‹åº ====================
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='æ‰¹é‡é‡å‘½åå·¥å…·')
    parser.add_argument('-d', '--dir', help='ç›®æ ‡ç›®å½•')
    parser.add_argument('-s', '--search', help='æŸ¥æ‰¾å†…å®¹')
    parser.add_argument('-r', '--replace', help='æ›¿æ¢å†…å®¹')
    parser.add_argument('--execute', action='store_true', help='æ‰§è¡Œé‡å‘½å')
    
    args = parser.parse_args()
    
    if args.dir:
        CONFIG['target_dir'] = args.dir
    if args.search:
        RENAME_RULES['search'] = args.search
    if args.replace:
        RENAME_RULES['replace'] = args.replace
    if args.execute:
        CONFIG['dry_run'] = False
    
    batch_rename(CONFIG, RENAME_RULES)
`;
  }

  private getLogAnalyzerTemplate(): string {
    return `#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ—¥å¿—åˆ†æå·¥å…·
# ç”¨æˆ·éœ€æ±‚: 
# ç”Ÿæˆæ—¶é—´: 
"""

import os
import re
from collections import Counter, defaultdict
from datetime import datetime
import argparse

# ==================== é…ç½®åŒºåŸŸ ====================
CONFIG = {
    'log_file': 'app.log',       # æ—¥å¿—æ–‡ä»¶è·¯å¾„
    'output_file': 'log_report.txt',  # æŠ¥å‘Šè¾“å‡ºè·¯å¾„
    'encoding': 'utf-8',         # æ–‡ä»¶ç¼–ç 
}

# æ—¥å¿—çº§åˆ«æ­£åˆ™
LOG_PATTERNS = {
    'error': r'\\b(ERROR|FATAL|CRITICAL)\\b',
    'warning': r'\\b(WARN|WARNING)\\b',
    'info': r'\\b(INFO)\\b',
    'debug': r'\\b(DEBUG)\\b',
}

# æ—¶é—´æˆ³æ­£åˆ™ï¼ˆæ ¹æ®æ—¥å¿—æ ¼å¼è°ƒæ•´ï¼‰
TIMESTAMP_PATTERN = r'(\\d{4}-\\d{2}-\\d{2}[T\\s]\\d{2}:\\d{2}:\\d{2})'

# ==================== åˆ†æå‡½æ•° ====================
def parse_log_line(line):
    """è§£ææ—¥å¿—è¡Œ"""
    result = {
        'timestamp': None,
        'level': 'unknown',
        'message': line.strip(),
    }
    
    # æå–æ—¶é—´æˆ³
    ts_match = re.search(TIMESTAMP_PATTERN, line)
    if ts_match:
        try:
            result['timestamp'] = datetime.fromisoformat(ts_match.group(1).replace(' ', 'T'))
        except:
            pass
    
    # è¯†åˆ«æ—¥å¿—çº§åˆ«
    for level, pattern in LOG_PATTERNS.items():
        if re.search(pattern, line, re.IGNORECASE):
            result['level'] = level
            break
    
    return result

def analyze_log(config):
    """åˆ†ææ—¥å¿—æ–‡ä»¶"""
    log_file = config['log_file']
    
    if not os.path.isfile(log_file):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {log_file}")
        return None
    
    stats = {
        'total_lines': 0,
        'level_counts': Counter(),
        'hourly_counts': defaultdict(int),
        'error_messages': [],
        'warning_messages': [],
    }
    
    print(f"åˆ†ææ—¥å¿—æ–‡ä»¶: {log_file}")
    
    with open(log_file, 'r', encoding=config['encoding'], errors='ignore') as f:
        for line in f:
            stats['total_lines'] += 1
            parsed = parse_log_line(line)
            
            stats['level_counts'][parsed['level']] += 1
            
            if parsed['timestamp']:
                hour_key = parsed['timestamp'].strftime('%Y-%m-%d %H:00')
                stats['hourly_counts'][hour_key] += 1
            
            if parsed['level'] == 'error':
                stats['error_messages'].append(parsed['message'][:200])
            elif parsed['level'] == 'warning':
                stats['warning_messages'].append(parsed['message'][:200])
    
    return stats

def generate_report(stats, output_file):
    """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
    report = []
    report.append("=" * 60)
    report.append("æ—¥å¿—åˆ†ææŠ¥å‘Š")
    report.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report.append("=" * 60)
    
    report.append(f"\\næ€»è¡Œæ•°: {stats['total_lines']}")
    
    report.append("\\næ—¥å¿—çº§åˆ«ç»Ÿè®¡:")
    for level, count in sorted(stats['level_counts'].items()):
        pct = count / stats['total_lines'] * 100 if stats['total_lines'] > 0 else 0
        report.append(f"  {level.upper():10} : {count:8} ({pct:.1f}%)")
    
    report.append("\\næ—¶é—´åˆ†å¸ƒ (æŒ‰å°æ—¶):")
    for hour, count in sorted(stats['hourly_counts'].items())[-24:]:  # æœ€è¿‘24å°æ—¶
        report.append(f"  {hour} : {count}")
    
    if stats['error_messages']:
        report.append(f"\\né”™è¯¯æ—¥å¿—ç¤ºä¾‹ (å‰10æ¡):")
        for msg in stats['error_messages'][:10]:
            report.append(f"  - {msg[:100]}...")
    
    report.append("\\n" + "=" * 60)
    
    report_text = "\\n".join(report)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(report_text)
    
    print(report_text)
    print(f"\\nâœ… æŠ¥å‘Šå·²ä¿å­˜è‡³: {output_file}")

# ==================== ä¸»ç¨‹åº ====================
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='æ—¥å¿—åˆ†æå·¥å…·')
    parser.add_argument('log_file', nargs='?', help='æ—¥å¿—æ–‡ä»¶è·¯å¾„')
    parser.add_argument('-o', '--output', help='æŠ¥å‘Šè¾“å‡ºè·¯å¾„')
    
    args = parser.parse_args()
    
    if args.log_file:
        CONFIG['log_file'] = args.log_file
    if args.output:
        CONFIG['output_file'] = args.output
    
    stats = analyze_log(CONFIG)
    if stats:
        generate_report(stats, CONFIG['output_file'])
`;
  }

  private getImageProcessorTemplate(): string {
    return `#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å›¾ç‰‡æ‰¹å¤„ç†å·¥å…·
# ç”¨æˆ·éœ€æ±‚: 
# ç”Ÿæˆæ—¶é—´: 
ä¾èµ–: pip install Pillow
"""

import os
from PIL import Image
import argparse

# ==================== é…ç½®åŒºåŸŸ ====================
CONFIG = {
    'input_dir': '.',            # è¾“å…¥ç›®å½•
    'output_dir': './processed', # è¾“å‡ºç›®å½•
    'formats': ['.jpg', '.jpeg', '.png', '.gif', '.bmp', '.webp'],
}

# å¤„ç†é€‰é¡¹
PROCESS_OPTIONS = {
    'resize': {
        'enabled': False,
        'width': 1920,
        'height': 1080,
        'keep_ratio': True,      # ä¿æŒå®½é«˜æ¯”
    },
    'compress': {
        'enabled': True,
        'quality': 85,           # JPEGè´¨é‡ (1-100)
    },
    'convert': {
        'enabled': False,
        'format': 'JPEG',        # ç›®æ ‡æ ¼å¼
    },
    'watermark': {
        'enabled': False,
        'text': 'Â© Copyright',
        'position': 'bottom-right',  # top-left/top-right/bottom-left/bottom-right/center
        'opacity': 128,          # é€æ˜åº¦ (0-255)
    },
}

# ==================== å¤„ç†å‡½æ•° ====================
def process_image(img, options):
    """å¤„ç†å•å¼ å›¾ç‰‡"""
    # è°ƒæ•´å°ºå¯¸
    if options['resize']['enabled']:
        target_w = options['resize']['width']
        target_h = options['resize']['height']
        
        if options['resize']['keep_ratio']:
            img.thumbnail((target_w, target_h), Image.Resampling.LANCZOS)
        else:
            img = img.resize((target_w, target_h), Image.Resampling.LANCZOS)
    
    # æ·»åŠ æ°´å°
    if options['watermark']['enabled']:
        from PIL import ImageDraw, ImageFont
        
        draw = ImageDraw.Draw(img)
        text = options['watermark']['text']
        
        # è®¡ç®—ä½ç½®
        try:
            font = ImageFont.truetype("arial.ttf", 24)
        except:
            font = ImageFont.load_default()
        
        bbox = draw.textbbox((0, 0), text, font=font)
        text_w, text_h = bbox[2] - bbox[0], bbox[3] - bbox[1]
        img_w, img_h = img.size
        
        positions = {
            'top-left': (10, 10),
            'top-right': (img_w - text_w - 10, 10),
            'bottom-left': (10, img_h - text_h - 10),
            'bottom-right': (img_w - text_w - 10, img_h - text_h - 10),
            'center': ((img_w - text_w) // 2, (img_h - text_h) // 2),
        }
        
        pos = positions.get(options['watermark']['position'], positions['bottom-right'])
        draw.text(pos, text, fill=(255, 255, 255, options['watermark']['opacity']), font=font)
    
    return img

def batch_process(config, options):
    """æ‰¹é‡å¤„ç†å›¾ç‰‡"""
    input_dir = config['input_dir']
    output_dir = config['output_dir']
    
    if not os.path.isdir(input_dir):
        print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {input_dir}")
        return
    
    os.makedirs(output_dir, exist_ok=True)
    
    # è·å–å›¾ç‰‡æ–‡ä»¶
    files = [f for f in os.listdir(input_dir) 
             if os.path.splitext(f)[1].lower() in config['formats']]
    
    print(f"æ‰¾åˆ° {len(files)} å¼ å›¾ç‰‡")
    print("-" * 50)
    
    processed = 0
    for filename in files:
        input_path = os.path.join(input_dir, filename)
        
        try:
            with Image.open(input_path) as img:
                # è½¬æ¢ä¸ºRGBï¼ˆå¦‚æœéœ€è¦ä¿å­˜ä¸ºJPEGï¼‰
                if img.mode in ('RGBA', 'P') and options['convert'].get('format') == 'JPEG':
                    img = img.convert('RGB')
                
                # å¤„ç†å›¾ç‰‡
                processed_img = process_image(img.copy(), options)
                
                # ç¡®å®šè¾“å‡ºæ–‡ä»¶åå’Œæ ¼å¼
                name, ext = os.path.splitext(filename)
                if options['convert']['enabled']:
                    ext = '.' + options['convert']['format'].lower()
                
                output_path = os.path.join(output_dir, name + ext)
                
                # ä¿å­˜
                save_kwargs = {}
                if ext.lower() in ['.jpg', '.jpeg']:
                    save_kwargs['quality'] = options['compress']['quality']
                    save_kwargs['optimize'] = True
                
                processed_img.save(output_path, **save_kwargs)
                
                # ç»Ÿè®¡
                orig_size = os.path.getsize(input_path)
                new_size = os.path.getsize(output_path)
                ratio = (1 - new_size / orig_size) * 100 if orig_size > 0 else 0
                
                print(f"  âœ“ {filename}")
                print(f"    {orig_size/1024:.1f}KB -> {new_size/1024:.1f}KB ({ratio:.1f}% èŠ‚çœ)")
                processed += 1
                
        except Exception as e:
            print(f"  âŒ {filename}: {e}")
    
    print("-" * 50)
    print(f"âœ… å¤„ç†å®Œæˆ: {processed}/{len(files)} å¼ å›¾ç‰‡")
    print(f"   è¾“å‡ºç›®å½•: {output_dir}")

# ==================== ä¸»ç¨‹åº ====================
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='å›¾ç‰‡æ‰¹å¤„ç†å·¥å…·')
    parser.add_argument('-i', '--input', help='è¾“å…¥ç›®å½•')
    parser.add_argument('-o', '--output', help='è¾“å‡ºç›®å½•')
    parser.add_argument('-q', '--quality', type=int, help='å‹ç¼©è´¨é‡ (1-100)')
    parser.add_argument('-w', '--width', type=int, help='ç›®æ ‡å®½åº¦')
    parser.add_argument('-H', '--height', type=int, help='ç›®æ ‡é«˜åº¦')
    
    args = parser.parse_args()
    
    if args.input:
        CONFIG['input_dir'] = args.input
    if args.output:
        CONFIG['output_dir'] = args.output
    if args.quality:
        PROCESS_OPTIONS['compress']['quality'] = args.quality
    if args.width:
        PROCESS_OPTIONS['resize']['enabled'] = True
        PROCESS_OPTIONS['resize']['width'] = args.width
    if args.height:
        PROCESS_OPTIONS['resize']['enabled'] = True
        PROCESS_OPTIONS['resize']['height'] = args.height
    
    batch_process(CONFIG, PROCESS_OPTIONS)
`;
  }

  private getFileOrganizerTemplate(): string {
    return `#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ–‡ä»¶æ•´ç†å·¥å…·
# ç”¨æˆ·éœ€æ±‚: 
# ç”Ÿæˆæ—¶é—´: 
"""

import os
import shutil
from datetime import datetime
from collections import defaultdict
import argparse

# ==================== é…ç½®åŒºåŸŸ ====================
CONFIG = {
    'source_dir': '.',           # æºç›®å½•
    'target_dir': './organized', # ç›®æ ‡ç›®å½•
    'mode': 'type',              # æ•´ç†æ¨¡å¼: type/date/size
    'dry_run': True,             # é¢„è§ˆæ¨¡å¼
    'move': False,               # True=ç§»åŠ¨, False=å¤åˆ¶
}

# æ–‡ä»¶ç±»å‹åˆ†ç±»
FILE_CATEGORIES = {
    'æ–‡æ¡£': ['.doc', '.docx', '.pdf', '.txt', '.md', '.rtf', '.odt', '.xls', '.xlsx', '.ppt', '.pptx'],
    'å›¾ç‰‡': ['.jpg', '.jpeg', '.png', '.gif', '.bmp', '.svg', '.webp', '.ico', '.tiff'],
    'è§†é¢‘': ['.mp4', '.avi', '.mkv', '.mov', '.wmv', '.flv', '.webm'],
    'éŸ³é¢‘': ['.mp3', '.wav', '.flac', '.aac', '.ogg', '.wma', '.m4a'],
    'å‹ç¼©åŒ…': ['.zip', '.rar', '.7z', '.tar', '.gz', '.bz2'],
    'ä»£ç ': ['.py', '.js', '.ts', '.java', '.c', '.cpp', '.h', '.go', '.rs', '.rb', '.php'],
    'æ•°æ®': ['.json', '.xml', '.yaml', '.yml', '.csv', '.sql', '.db'],
    'å¯æ‰§è¡Œ': ['.exe', '.msi', '.dmg', '.app', '.sh', '.bat'],
}

# æ–‡ä»¶å¤§å°åˆ†ç±»ï¼ˆå­—èŠ‚ï¼‰
SIZE_CATEGORIES = {
    'å°æ–‡ä»¶ (<1MB)': (0, 1024 * 1024),
    'ä¸­ç­‰æ–‡ä»¶ (1-100MB)': (1024 * 1024, 100 * 1024 * 1024),
    'å¤§æ–‡ä»¶ (>100MB)': (100 * 1024 * 1024, float('inf')),
}

# ==================== æ•´ç†å‡½æ•° ====================
def get_category_by_type(ext):
    """æ ¹æ®æ‰©å±•åè·å–åˆ†ç±»"""
    ext = ext.lower()
    for category, extensions in FILE_CATEGORIES.items():
        if ext in extensions:
            return category
    return 'å…¶ä»–'

def get_category_by_date(filepath):
    """æ ¹æ®ä¿®æ”¹æ—¥æœŸè·å–åˆ†ç±»"""
    mtime = os.path.getmtime(filepath)
    dt = datetime.fromtimestamp(mtime)
    return dt.strftime('%Y/%Y-%m')

def get_category_by_size(filepath):
    """æ ¹æ®æ–‡ä»¶å¤§å°è·å–åˆ†ç±»"""
    size = os.path.getsize(filepath)
    for category, (min_size, max_size) in SIZE_CATEGORIES.items():
        if min_size <= size < max_size:
            return category
    return 'å…¶ä»–'

def organize_files(config):
    """æ•´ç†æ–‡ä»¶"""
    source_dir = config['source_dir']
    target_dir = config['target_dir']
    mode = config['mode']
    
    if not os.path.isdir(source_dir):
        print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {source_dir}")
        return
    
    # è·å–æ‰€æœ‰æ–‡ä»¶
    files = []
    for item in os.listdir(source_dir):
        filepath = os.path.join(source_dir, item)
        if os.path.isfile(filepath):
            files.append(filepath)
    
    print(f"æ‰¾åˆ° {len(files)} ä¸ªæ–‡ä»¶")
    print(f"æ•´ç†æ¨¡å¼: {mode}")
    print("-" * 50)
    
    # åˆ†ç±»æ–‡ä»¶
    categories = defaultdict(list)
    for filepath in files:
        filename = os.path.basename(filepath)
        ext = os.path.splitext(filename)[1]
        
        if mode == 'type':
            category = get_category_by_type(ext)
        elif mode == 'date':
            category = get_category_by_date(filepath)
        elif mode == 'size':
            category = get_category_by_size(filepath)
        else:
            category = 'æœªåˆ†ç±»'
        
        categories[category].append(filepath)
    
    # æ‰§è¡Œæ•´ç†
    moved_count = 0
    for category, file_list in sorted(categories.items()):
        print(f"\\nğŸ“ {category}: {len(file_list)} ä¸ªæ–‡ä»¶")
        
        category_dir = os.path.join(target_dir, category)
        
        if not config['dry_run']:
            os.makedirs(category_dir, exist_ok=True)
        
        for filepath in file_list:
            filename = os.path.basename(filepath)
            dest_path = os.path.join(category_dir, filename)
            
            print(f"   {filename}")
            
            if not config['dry_run']:
                try:
                    if config['move']:
                        shutil.move(filepath, dest_path)
                    else:
                        shutil.copy2(filepath, dest_path)
                    moved_count += 1
                except Exception as e:
                    print(f"   âŒ å¤±è´¥: {e}")
    
    print("\\n" + "-" * 50)
    if config['dry_run']:
        print("âš ï¸  é¢„è§ˆæ¨¡å¼ï¼Œæœªå®é™…æ“ä½œ")
        print("   è®¾ç½® dry_run = False ä»¥æ‰§è¡Œæ“ä½œ")
    else:
        action = "ç§»åŠ¨" if config['move'] else "å¤åˆ¶"
        print(f"âœ… å·²{action} {moved_count}/{len(files)} ä¸ªæ–‡ä»¶")

# ==================== ä¸»ç¨‹åº ====================
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='æ–‡ä»¶æ•´ç†å·¥å…·')
    parser.add_argument('-s', '--source', help='æºç›®å½•')
    parser.add_argument('-t', '--target', help='ç›®æ ‡ç›®å½•')
    parser.add_argument('-m', '--mode', choices=['type', 'date', 'size'], help='æ•´ç†æ¨¡å¼')
    parser.add_argument('--move', action='store_true', help='ç§»åŠ¨æ–‡ä»¶ï¼ˆé»˜è®¤å¤åˆ¶ï¼‰')
    parser.add_argument('--execute', action='store_true', help='æ‰§è¡Œæ“ä½œ')
    
    args = parser.parse_args()
    
    if args.source:
        CONFIG['source_dir'] = args.source
    if args.target:
        CONFIG['target_dir'] = args.target
    if args.mode:
        CONFIG['mode'] = args.mode
    if args.move:
        CONFIG['move'] = True
    if args.execute:
        CONFIG['dry_run'] = False
    
    organize_files(CONFIG)
`;
  }

  private getDataConverterTemplate(): string {
    return `#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®æ ¼å¼è½¬æ¢å·¥å…·
# ç”¨æˆ·éœ€æ±‚: 
# ç”Ÿæˆæ—¶é—´: 
ä¾èµ–: pip install PyYAML
"""

import json
import csv
import xml.etree.ElementTree as ET
from xml.dom import minidom
import argparse
import os

try:
    import yaml
    HAS_YAML = True
except ImportError:
    HAS_YAML = False
    print("âš ï¸ PyYAMLæœªå®‰è£…ï¼ŒYAMLåŠŸèƒ½ä¸å¯ç”¨")

# ==================== è½¬æ¢å‡½æ•° ====================
def json_to_csv(input_file, output_file):
    """JSONè½¬CSV"""
    with open(input_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    if isinstance(data, dict):
        data = [data]
    
    if not data:
        print("âŒ æ•°æ®ä¸ºç©º")
        return
    
    with open(output_file, 'w', encoding='utf-8-sig', newline='') as f:
        writer = csv.DictWriter(f, fieldnames=data[0].keys())
        writer.writeheader()
        writer.writerows(data)
    
    print(f"âœ… å·²è½¬æ¢: {output_file}")

def csv_to_json(input_file, output_file):
    """CSVè½¬JSON"""
    data = []
    with open(input_file, 'r', encoding='utf-8-sig') as f:
        reader = csv.DictReader(f)
        for row in reader:
            data.append(row)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… å·²è½¬æ¢: {output_file}")

def json_to_xml(input_file, output_file):
    """JSONè½¬XML"""
    with open(input_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    def dict_to_xml(d, root_name='root'):
        root = ET.Element(root_name)
        
        def add_element(parent, data):
            if isinstance(data, dict):
                for key, value in data.items():
                    child = ET.SubElement(parent, str(key))
                    add_element(child, value)
            elif isinstance(data, list):
                for item in data:
                    child = ET.SubElement(parent, 'item')
                    add_element(child, item)
            else:
                parent.text = str(data) if data is not None else ''
        
        add_element(root, data)
        return root
    
    root = dict_to_xml(data)
    xml_str = minidom.parseString(ET.tostring(root)).toprettyxml(indent="  ")
    
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(xml_str)
    
    print(f"âœ… å·²è½¬æ¢: {output_file}")

def json_to_yaml(input_file, output_file):
    """JSONè½¬YAML"""
    if not HAS_YAML:
        print("âŒ è¯·å®‰è£…PyYAML: pip install PyYAML")
        return
    
    with open(input_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        yaml.dump(data, f, allow_unicode=True, default_flow_style=False)
    
    print(f"âœ… å·²è½¬æ¢: {output_file}")

def yaml_to_json(input_file, output_file):
    """YAMLè½¬JSON"""
    if not HAS_YAML:
        print("âŒ è¯·å®‰è£…PyYAML: pip install PyYAML")
        return
    
    with open(input_file, 'r', encoding='utf-8') as f:
        data = yaml.safe_load(f)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… å·²è½¬æ¢: {output_file}")

# ==================== ä¸»ç¨‹åº ====================
CONVERTERS = {
    ('json', 'csv'): json_to_csv,
    ('csv', 'json'): csv_to_json,
    ('json', 'xml'): json_to_xml,
    ('json', 'yaml'): json_to_yaml,
    ('yaml', 'json'): yaml_to_json,
}

def convert(input_file, output_format):
    """è‡ªåŠ¨æ£€æµ‹å¹¶è½¬æ¢"""
    if not os.path.isfile(input_file):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
        return
    
    input_ext = os.path.splitext(input_file)[1].lower().lstrip('.')
    if input_ext in ['yml']:
        input_ext = 'yaml'
    
    converter = CONVERTERS.get((input_ext, output_format))
    if not converter:
        print(f"âŒ ä¸æ”¯æŒçš„è½¬æ¢: {input_ext} -> {output_format}")
        print(f"   æ”¯æŒçš„è½¬æ¢: {list(CONVERTERS.keys())}")
        return
    
    output_file = os.path.splitext(input_file)[0] + '.' + output_format
    converter(input_file, output_file)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='æ•°æ®æ ¼å¼è½¬æ¢å·¥å…·')
    parser.add_argument('input', help='è¾“å…¥æ–‡ä»¶')
    parser.add_argument('-f', '--format', required=True, 
                        choices=['json', 'csv', 'xml', 'yaml'],
                        help='è¾“å‡ºæ ¼å¼')
    
    args = parser.parse_args()
    convert(args.input, args.format)
`;
  }

  private getBackupToolTemplate(): string {
    return `#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ–‡ä»¶å¤‡ä»½å·¥å…·
# ç”¨æˆ·éœ€æ±‚: 
# ç”Ÿæˆæ—¶é—´: 
"""

import os
import shutil
import zipfile
import hashlib
from datetime import datetime
import json
import argparse

# ==================== é…ç½®åŒºåŸŸ ====================
CONFIG = {
    'source_dirs': ['.'],        # è¦å¤‡ä»½çš„ç›®å½•åˆ—è¡¨
    'backup_dir': './backups',   # å¤‡ä»½å­˜å‚¨ç›®å½•
    'exclude_patterns': [        # æ’é™¤çš„æ–‡ä»¶/ç›®å½•
        '__pycache__',
        '.git',
        'node_modules',
        '.venv',
        '*.pyc',
        '*.log',
    ],
    'compress': True,            # æ˜¯å¦å‹ç¼©
    'incremental': False,        # å¢é‡å¤‡ä»½
    'max_backups': 10,           # æœ€å¤§ä¿ç•™å¤‡ä»½æ•°
}

# ==================== å¤‡ä»½å‡½æ•° ====================
def should_exclude(path, patterns):
    """æ£€æŸ¥æ˜¯å¦åº”è¯¥æ’é™¤"""
    name = os.path.basename(path)
    for pattern in patterns:
        if pattern.startswith('*'):
            if name.endswith(pattern[1:]):
                return True
        elif pattern in path:
            return True
    return False

def get_file_hash(filepath):
    """è®¡ç®—æ–‡ä»¶MD5"""
    hasher = hashlib.md5()
    with open(filepath, 'rb') as f:
        for chunk in iter(lambda: f.read(8192), b''):
            hasher.update(chunk)
    return hasher.hexdigest()

def collect_files(source_dirs, exclude_patterns):
    """æ”¶é›†è¦å¤‡ä»½çš„æ–‡ä»¶"""
    files = []
    for source_dir in source_dirs:
        if not os.path.isdir(source_dir):
            print(f"âš ï¸ ç›®å½•ä¸å­˜åœ¨: {source_dir}")
            continue
        
        for root, dirs, filenames in os.walk(source_dir):
            # æ’é™¤ç›®å½•
            dirs[:] = [d for d in dirs if not should_exclude(os.path.join(root, d), exclude_patterns)]
            
            for filename in filenames:
                filepath = os.path.join(root, filename)
                if not should_exclude(filepath, exclude_patterns):
                    files.append(filepath)
    
    return files

def create_backup(config):
    """åˆ›å»ºå¤‡ä»½"""
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    backup_name = f"backup_{timestamp}"
    backup_dir = config['backup_dir']
    
    os.makedirs(backup_dir, exist_ok=True)
    
    # æ”¶é›†æ–‡ä»¶
    files = collect_files(config['source_dirs'], config['exclude_patterns'])
    print(f"æ‰¾åˆ° {len(files)} ä¸ªæ–‡ä»¶")
    
    if not files:
        print("âŒ æ²¡æœ‰è¦å¤‡ä»½çš„æ–‡ä»¶")
        return
    
    # åŠ è½½ä¸Šæ¬¡å¤‡ä»½çš„å“ˆå¸Œï¼ˆç”¨äºå¢é‡å¤‡ä»½ï¼‰
    hash_file = os.path.join(backup_dir, 'last_backup_hashes.json')
    last_hashes = {}
    if config['incremental'] and os.path.isfile(hash_file):
        with open(hash_file, 'r') as f:
            last_hashes = json.load(f)
    
    # è¿‡æ»¤éœ€è¦å¤‡ä»½çš„æ–‡ä»¶
    current_hashes = {}
    files_to_backup = []
    for filepath in files:
        file_hash = get_file_hash(filepath)
        current_hashes[filepath] = file_hash
        
        if not config['incremental'] or last_hashes.get(filepath) != file_hash:
            files_to_backup.append(filepath)
    
    if config['incremental']:
        print(f"å¢é‡å¤‡ä»½: {len(files_to_backup)} ä¸ªæ–‡ä»¶æœ‰å˜åŒ–")
    
    if not files_to_backup:
        print("âœ… æ‰€æœ‰æ–‡ä»¶éƒ½æ˜¯æœ€æ–°çš„ï¼Œæ— éœ€å¤‡ä»½")
        return
    
    # åˆ›å»ºå¤‡ä»½
    if config['compress']:
        backup_path = os.path.join(backup_dir, f"{backup_name}.zip")
        with zipfile.ZipFile(backup_path, 'w', zipfile.ZIP_DEFLATED) as zf:
            for filepath in files_to_backup:
                zf.write(filepath)
                print(f"  + {filepath}")
    else:
        backup_path = os.path.join(backup_dir, backup_name)
        os.makedirs(backup_path, exist_ok=True)
        for filepath in files_to_backup:
            dest = os.path.join(backup_path, filepath)
            os.makedirs(os.path.dirname(dest), exist_ok=True)
            shutil.copy2(filepath, dest)
            print(f"  + {filepath}")
    
    # ä¿å­˜å½“å‰å“ˆå¸Œ
    with open(hash_file, 'w') as f:
        json.dump(current_hashes, f)
    
    # æ¸…ç†æ—§å¤‡ä»½
    cleanup_old_backups(backup_dir, config['max_backups'])
    
    # ç»Ÿè®¡
    backup_size = os.path.getsize(backup_path) if config['compress'] else sum(
        os.path.getsize(os.path.join(backup_path, f)) 
        for f in os.listdir(backup_path)
    )
    
    print(f"\\nâœ… å¤‡ä»½å®Œæˆ: {backup_path}")
    print(f"   æ–‡ä»¶æ•°: {len(files_to_backup)}")
    print(f"   å¤§å°: {backup_size / 1024 / 1024:.2f} MB")

def cleanup_old_backups(backup_dir, max_backups):
    """æ¸…ç†æ—§å¤‡ä»½"""
    backups = sorted([
        f for f in os.listdir(backup_dir) 
        if f.startswith('backup_') and (f.endswith('.zip') or os.path.isdir(os.path.join(backup_dir, f)))
    ])
    
    while len(backups) > max_backups:
        old_backup = backups.pop(0)
        old_path = os.path.join(backup_dir, old_backup)
        if os.path.isfile(old_path):
            os.remove(old_path)
        else:
            shutil.rmtree(old_path)
        print(f"  ğŸ—‘ï¸ åˆ é™¤æ—§å¤‡ä»½: {old_backup}")

# ==================== ä¸»ç¨‹åº ====================
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='æ–‡ä»¶å¤‡ä»½å·¥å…·')
    parser.add_argument('-s', '--source', nargs='+', help='æºç›®å½•')
    parser.add_argument('-d', '--dest', help='å¤‡ä»½ç›®å½•')
    parser.add_argument('-i', '--incremental', action='store_true', help='å¢é‡å¤‡ä»½')
    parser.add_argument('--no-compress', action='store_true', help='ä¸å‹ç¼©')
    
    args = parser.parse_args()
    
    if args.source:
        CONFIG['source_dirs'] = args.source
    if args.dest:
        CONFIG['backup_dir'] = args.dest
    if args.incremental:
        CONFIG['incremental'] = True
    if args.no_compress:
        CONFIG['compress'] = False
    
    create_backup(CONFIG)
`;
  }

  private getTextProcessorTemplate(): string {
    return `#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ–‡æœ¬å¤„ç†å·¥å…·
# ç”¨æˆ·éœ€æ±‚: 
# ç”Ÿæˆæ—¶é—´: 
ä¾èµ–: pip install chardet (å¯é€‰ï¼Œç”¨äºç¼–ç æ£€æµ‹)
"""

import os
import re
import argparse

try:
    import chardet
    HAS_CHARDET = True
except ImportError:
    HAS_CHARDET = False

# ==================== é…ç½®åŒºåŸŸ ====================
CONFIG = {
    'target_dir': '.',
    'file_patterns': ['*.txt', '*.md', '*.csv'],
    'recursive': False,
    'dry_run': True,
}

# å¤„ç†é€‰é¡¹
PROCESS_OPTIONS = {
    'find_replace': {
        'enabled': False,
        'rules': [
            # {'find': 'æ—§å†…å®¹', 'replace': 'æ–°å†…å®¹', 'regex': False},
        ],
    },
    'encoding': {
        'enabled': False,
        'target_encoding': 'utf-8',
    },
    'line_endings': {
        'enabled': False,
        'target': 'unix',  # unix(LF) / windows(CRLF)
    },
    'trim': {
        'enabled': False,
        'trailing_whitespace': True,
        'empty_lines': True,
    },
}

# ==================== å¤„ç†å‡½æ•° ====================
def detect_encoding(filepath):
    """æ£€æµ‹æ–‡ä»¶ç¼–ç """
    if not HAS_CHARDET:
        return 'utf-8'
    
    with open(filepath, 'rb') as f:
        raw = f.read(10000)
    result = chardet.detect(raw)
    return result['encoding'] or 'utf-8'

def process_text(content, options):
    """å¤„ç†æ–‡æœ¬å†…å®¹"""
    modified = False
    
    # æŸ¥æ‰¾æ›¿æ¢
    if options['find_replace']['enabled']:
        for rule in options['find_replace']['rules']:
            if rule.get('regex'):
                new_content = re.sub(rule['find'], rule['replace'], content)
            else:
                new_content = content.replace(rule['find'], rule['replace'])
            if new_content != content:
                modified = True
                content = new_content
    
    # å¤„ç†è¡Œå°¾
    if options['line_endings']['enabled']:
        if options['line_endings']['target'] == 'unix':
            new_content = content.replace('\\r\\n', '\\n')
        else:
            new_content = content.replace('\\r\\n', '\\n').replace('\\n', '\\r\\n')
        if new_content != content:
            modified = True
            content = new_content
    
    # æ¸…ç†ç©ºç™½
    if options['trim']['enabled']:
        lines = content.split('\\n')
        
        if options['trim']['trailing_whitespace']:
            lines = [line.rstrip() for line in lines]
            modified = True
        
        if options['trim']['empty_lines']:
            # ç§»é™¤è¿ç»­ç©ºè¡Œ
            new_lines = []
            prev_empty = False
            for line in lines:
                is_empty = len(line.strip()) == 0
                if not (is_empty and prev_empty):
                    new_lines.append(line)
                prev_empty = is_empty
            lines = new_lines
            modified = True
        
        content = '\\n'.join(lines)
    
    return content, modified

def batch_process(config, options):
    """æ‰¹é‡å¤„ç†æ–‡æœ¬æ–‡ä»¶"""
    import fnmatch
    
    target_dir = config['target_dir']
    
    if not os.path.isdir(target_dir):
        print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {target_dir}")
        return
    
    # æ”¶é›†æ–‡ä»¶
    files = []
    if config['recursive']:
        for root, _, filenames in os.walk(target_dir):
            for pattern in config['file_patterns']:
                for filename in fnmatch.filter(filenames, pattern):
                    files.append(os.path.join(root, filename))
    else:
        for filename in os.listdir(target_dir):
            filepath = os.path.join(target_dir, filename)
            if os.path.isfile(filepath):
                for pattern in config['file_patterns']:
                    if fnmatch.fnmatch(filename, pattern):
                        files.append(filepath)
                        break
    
    print(f"æ‰¾åˆ° {len(files)} ä¸ªæ–‡ä»¶")
    print("-" * 50)
    
    processed = 0
    for filepath in files:
        try:
            # æ£€æµ‹ç¼–ç 
            encoding = detect_encoding(filepath)
            
            # è¯»å–å†…å®¹
            with open(filepath, 'r', encoding=encoding, errors='replace') as f:
                content = f.read()
            
            # å¤„ç†å†…å®¹
            new_content, modified = process_text(content, options)
            
            # è½¬æ¢ç¼–ç 
            target_encoding = encoding
            if options['encoding']['enabled']:
                target_encoding = options['encoding']['target_encoding']
                if encoding != target_encoding:
                    modified = True
            
            if modified:
                print(f"  âœ“ {filepath}")
                if encoding != target_encoding:
                    print(f"    ç¼–ç : {encoding} -> {target_encoding}")
                
                if not config['dry_run']:
                    with open(filepath, 'w', encoding=target_encoding) as f:
                        f.write(new_content)
                processed += 1
            else:
                print(f"  - {filepath} (æ— å˜åŒ–)")
                
        except Exception as e:
            print(f"  âŒ {filepath}: {e}")
    
    print("-" * 50)
    if config['dry_run']:
        print("âš ï¸  é¢„è§ˆæ¨¡å¼ï¼Œæœªå®é™…ä¿®æ”¹")
    else:
        print(f"âœ… å·²å¤„ç† {processed} ä¸ªæ–‡ä»¶")

# ==================== ä¸»ç¨‹åº ====================
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='æ–‡æœ¬å¤„ç†å·¥å…·')
    parser.add_argument('-d', '--dir', help='ç›®æ ‡ç›®å½•')
    parser.add_argument('-f', '--find', help='æŸ¥æ‰¾å†…å®¹')
    parser.add_argument('-r', '--replace', help='æ›¿æ¢å†…å®¹')
    parser.add_argument('-e', '--encoding', help='ç›®æ ‡ç¼–ç ')
    parser.add_argument('--execute', action='store_true', help='æ‰§è¡Œä¿®æ”¹')
    
    args = parser.parse_args()
    
    if args.dir:
        CONFIG['target_dir'] = args.dir
    if args.find:
        PROCESS_OPTIONS['find_replace']['enabled'] = True
        PROCESS_OPTIONS['find_replace']['rules'].append({
            'find': args.find,
            'replace': args.replace or '',
            'regex': False,
        })
    if args.encoding:
        PROCESS_OPTIONS['encoding']['enabled'] = True
        PROCESS_OPTIONS['encoding']['target_encoding'] = args.encoding
    if args.execute:
        CONFIG['dry_run'] = False
    
    batch_process(CONFIG, PROCESS_OPTIONS)
`;
  }

  private getAPITesterTemplate(): string {
    return `#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
APIæµ‹è¯•å·¥å…·
# ç”¨æˆ·éœ€æ±‚: 
# ç”Ÿæˆæ—¶é—´: 
ä¾èµ–: pip install requests
"""

import json
import time
from datetime import datetime
import argparse

try:
    import requests
    HAS_REQUESTS = True
except ImportError:
    HAS_REQUESTS = False
    print("âŒ è¯·å®‰è£…requests: pip install requests")

# ==================== é…ç½®åŒºåŸŸ ====================
API_TESTS = [
    {
        'name': 'ç¤ºä¾‹GETè¯·æ±‚',
        'method': 'GET',
        'url': 'https://httpbin.org/get',
        'headers': {},
        'params': {'key': 'value'},
        'expected_status': 200,
    },
    {
        'name': 'ç¤ºä¾‹POSTè¯·æ±‚',
        'method': 'POST',
        'url': 'https://httpbin.org/post',
        'headers': {'Content-Type': 'application/json'},
        'body': {'message': 'Hello'},
        'expected_status': 200,
    },
]

CONFIG = {
    'timeout': 30,
    'retry': 2,
    'delay': 1,  # è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰
    'verbose': True,
}

# ==================== æµ‹è¯•å‡½æ•° ====================
def run_test(test_case, config):
    """æ‰§è¡Œå•ä¸ªæµ‹è¯•"""
    result = {
        'name': test_case['name'],
        'success': False,
        'status_code': None,
        'response_time': None,
        'error': None,
    }
    
    method = test_case.get('method', 'GET').upper()
    url = test_case['url']
    headers = test_case.get('headers', {})
    params = test_case.get('params', {})
    body = test_case.get('body')
    expected_status = test_case.get('expected_status', 200)
    
    for attempt in range(config['retry'] + 1):
        try:
            start_time = time.time()
            
            if method == 'GET':
                response = requests.get(url, headers=headers, params=params, timeout=config['timeout'])
            elif method == 'POST':
                response = requests.post(url, headers=headers, json=body, timeout=config['timeout'])
            elif method == 'PUT':
                response = requests.put(url, headers=headers, json=body, timeout=config['timeout'])
            elif method == 'DELETE':
                response = requests.delete(url, headers=headers, timeout=config['timeout'])
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ–¹æ³•: {method}")
            
            result['response_time'] = (time.time() - start_time) * 1000  # ms
            result['status_code'] = response.status_code
            result['success'] = response.status_code == expected_status
            
            if config['verbose']:
                print(f"  å“åº”çŠ¶æ€: {response.status_code}")
                print(f"  å“åº”æ—¶é—´: {result['response_time']:.0f}ms")
            
            break
            
        except Exception as e:
            result['error'] = str(e)
            if attempt < config['retry']:
                print(f"  é‡è¯• ({attempt + 1}/{config['retry']})...")
                time.sleep(1)
            else:
                print(f"  âŒ å¤±è´¥: {e}")
    
    return result

def run_all_tests(tests, config):
    """æ‰§è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("=" * 60)
    print("APIæµ‹è¯•æŠ¥å‘Š")
    print(f"æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)
    
    results = []
    passed = 0
    failed = 0
    
    for i, test in enumerate(tests, 1):
        print(f"\\n[{i}/{len(tests)}] {test['name']}")
        print(f"  {test.get('method', 'GET')} {test['url']}")
        
        result = run_test(test, config)
        results.append(result)
        
        if result['success']:
            passed += 1
            print("  âœ… é€šè¿‡")
        else:
            failed += 1
            print("  âŒ å¤±è´¥")
        
        if config['delay'] and i < len(tests):
            time.sleep(config['delay'])
    
    # æ±‡æ€»
    print("\\n" + "=" * 60)
    print(f"æµ‹è¯•å®Œæˆ: {passed}/{len(tests)} é€šè¿‡")
    
    if results:
        avg_time = sum(r['response_time'] or 0 for r in results) / len(results)
        print(f"å¹³å‡å“åº”æ—¶é—´: {avg_time:.0f}ms")
    
    print("=" * 60)
    
    return results

# ==================== ä¸»ç¨‹åº ====================
if __name__ == "__main__":
    if not HAS_REQUESTS:
        exit(1)
    
    parser = argparse.ArgumentParser(description='APIæµ‹è¯•å·¥å…·')
    parser.add_argument('-u', '--url', help='æµ‹è¯•URL')
    parser.add_argument('-m', '--method', default='GET', help='è¯·æ±‚æ–¹æ³•')
    parser.add_argument('-d', '--data', help='è¯·æ±‚æ•°æ®(JSON)')
    
    args = parser.parse_args()
    
    if args.url:
        # å•ä¸ªæµ‹è¯•
        test = {
            'name': 'å‘½ä»¤è¡Œæµ‹è¯•',
            'method': args.method,
            'url': args.url,
        }
        if args.data:
            test['body'] = json.loads(args.data)
        
        API_TESTS = [test]
    
    run_all_tests(API_TESTS, CONFIG)
`;
  }

  private getPortScannerTemplate(): string {
    return `#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç«¯å£æ‰«æå·¥å…·
# ç”¨æˆ·éœ€æ±‚: 
# ç”Ÿæˆæ—¶é—´: 
"""

import socket
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
import argparse

# ==================== é…ç½®åŒºåŸŸ ====================
CONFIG = {
    'host': 'localhost',
    'ports': range(1, 1025),     # æ‰«æç«¯å£èŒƒå›´
    'timeout': 1,                 # è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    'threads': 100,               # å¹¶å‘çº¿ç¨‹æ•°
}

# å¸¸è§ç«¯å£æœåŠ¡
COMMON_PORTS = {
    21: 'FTP',
    22: 'SSH',
    23: 'Telnet',
    25: 'SMTP',
    53: 'DNS',
    80: 'HTTP',
    110: 'POP3',
    143: 'IMAP',
    443: 'HTTPS',
    445: 'SMB',
    3306: 'MySQL',
    3389: 'RDP',
    5432: 'PostgreSQL',
    6379: 'Redis',
    8080: 'HTTP-Alt',
    27017: 'MongoDB',
}

# ==================== æ‰«æå‡½æ•° ====================
def scan_port(host, port, timeout):
    """æ‰«æå•ä¸ªç«¯å£"""
    try:
        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        sock.settimeout(timeout)
        result = sock.connect_ex((host, port))
        sock.close()
        return port, result == 0
    except:
        return port, False

def scan_host(config):
    """æ‰«æä¸»æœºç«¯å£"""
    host = config['host']
    ports = list(config['ports'])
    
    print(f"æ‰«æä¸»æœº: {host}")
    print(f"ç«¯å£èŒƒå›´: {ports[0]}-{ports[-1]}")
    print(f"çº¿ç¨‹æ•°: {config['threads']}")
    print("-" * 50)
    
    open_ports = []
    scanned = 0
    
    with ThreadPoolExecutor(max_workers=config['threads']) as executor:
        futures = {
            executor.submit(scan_port, host, port, config['timeout']): port 
            for port in ports
        }
        
        for future in as_completed(futures):
            port, is_open = future.result()
            scanned += 1
            
            if is_open:
                service = COMMON_PORTS.get(port, 'unknown')
                open_ports.append((port, service))
                print(f"  âœ“ ç«¯å£ {port:5} å¼€æ”¾ ({service})")
            
            # è¿›åº¦æ˜¾ç¤º
            if scanned % 100 == 0:
                print(f"  å·²æ‰«æ: {scanned}/{len(ports)}", end='\\r')
    
    print("\\n" + "-" * 50)
    print(f"æ‰«æå®Œæˆ: {len(ports)} ä¸ªç«¯å£")
    print(f"å¼€æ”¾ç«¯å£: {len(open_ports)} ä¸ª")
    
    if open_ports:
        print("\\nå¼€æ”¾ç«¯å£åˆ—è¡¨:")
        for port, service in sorted(open_ports):
            print(f"  {port:5} - {service}")
    
    return open_ports

# ==================== ä¸»ç¨‹åº ====================
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='ç«¯å£æ‰«æå·¥å…·')
    parser.add_argument('host', nargs='?', default='localhost', help='ç›®æ ‡ä¸»æœº')
    parser.add_argument('-p', '--ports', help='ç«¯å£èŒƒå›´ï¼Œå¦‚: 1-1024 æˆ– 80,443,8080')
    parser.add_argument('-t', '--threads', type=int, help='çº¿ç¨‹æ•°')
    
    args = parser.parse_args()
    
    CONFIG['host'] = args.host
    
    if args.ports:
        if '-' in args.ports:
            start, end = map(int, args.ports.split('-'))
            CONFIG['ports'] = range(start, end + 1)
        elif ',' in args.ports:
            CONFIG['ports'] = [int(p) for p in args.ports.split(',')]
    
    if args.threads:
        CONFIG['threads'] = args.threads
    
    scan_host(CONFIG)
`;
  }

  private getSystemMonitorTemplate(): string {
    return `#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç³»ç»Ÿç›‘æ§å·¥å…·
# ç”¨æˆ·éœ€æ±‚: 
# ç”Ÿæˆæ—¶é—´: 
ä¾èµ–: pip install psutil
"""

import time
from datetime import datetime
import argparse

try:
    import psutil
    HAS_PSUTIL = True
except ImportError:
    HAS_PSUTIL = False
    print("âŒ è¯·å®‰è£…psutil: pip install psutil")

# ==================== é…ç½®åŒºåŸŸ ====================
CONFIG = {
    'interval': 2,       # åˆ·æ–°é—´éš”ï¼ˆç§’ï¼‰
    'duration': 0,       # ç›‘æ§æ—¶é•¿ï¼ˆç§’ï¼‰ï¼Œ0è¡¨ç¤ºæŒç»­ç›‘æ§
    'show_processes': 5, # æ˜¾ç¤ºtop Nè¿›ç¨‹
}

# ==================== ç›‘æ§å‡½æ•° ====================
def get_size(bytes):
    """æ ¼å¼åŒ–å­—èŠ‚å¤§å°"""
    for unit in ['B', 'KB', 'MB', 'GB', 'TB']:
        if bytes < 1024:
            return f"{bytes:.1f}{unit}"
        bytes /= 1024

def get_system_info():
    """è·å–ç³»ç»Ÿä¿¡æ¯"""
    info = {}
    
    # CPU
    info['cpu_percent'] = psutil.cpu_percent(interval=1)
    info['cpu_count'] = psutil.cpu_count()
    info['cpu_freq'] = psutil.cpu_freq()
    
    # å†…å­˜
    mem = psutil.virtual_memory()
    info['mem_total'] = mem.total
    info['mem_used'] = mem.used
    info['mem_percent'] = mem.percent
    
    # ç£ç›˜
    disk = psutil.disk_usage('/')
    info['disk_total'] = disk.total
    info['disk_used'] = disk.used
    info['disk_percent'] = disk.percent
    
    # ç½‘ç»œ
    net = psutil.net_io_counters()
    info['net_sent'] = net.bytes_sent
    info['net_recv'] = net.bytes_recv
    
    return info

def get_top_processes(n):
    """è·å–CPUå ç”¨æœ€é«˜çš„è¿›ç¨‹"""
    processes = []
    for proc in psutil.process_iter(['pid', 'name', 'cpu_percent', 'memory_percent']):
        try:
            processes.append(proc.info)
        except:
            pass
    
    return sorted(processes, key=lambda x: x['cpu_percent'] or 0, reverse=True)[:n]

def print_monitor(info, processes, prev_net=None):
    """æ‰“å°ç›‘æ§ä¿¡æ¯"""
    # æ¸…å±
    print("\\033[2J\\033[H", end='')
    
    print("=" * 60)
    print(f"ç³»ç»Ÿç›‘æ§ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)
    
    # CPU
    print(f"\\nğŸ“Š CPU")
    print(f"   ä½¿ç”¨ç‡: {info['cpu_percent']:5.1f}%")
    print(f"   æ ¸å¿ƒæ•°: {info['cpu_count']}")
    if info['cpu_freq']:
        print(f"   é¢‘ç‡: {info['cpu_freq'].current:.0f} MHz")
    
    # è¿›åº¦æ¡
    bar_width = 30
    filled = int(bar_width * info['cpu_percent'] / 100)
    bar = 'â–ˆ' * filled + 'â–‘' * (bar_width - filled)
    print(f"   [{bar}] {info['cpu_percent']:.1f}%")
    
    # å†…å­˜
    print(f"\\nğŸ’¾ å†…å­˜")
    print(f"   ä½¿ç”¨: {get_size(info['mem_used'])} / {get_size(info['mem_total'])}")
    filled = int(bar_width * info['mem_percent'] / 100)
    bar = 'â–ˆ' * filled + 'â–‘' * (bar_width - filled)
    print(f"   [{bar}] {info['mem_percent']:.1f}%")
    
    # ç£ç›˜
    print(f"\\nğŸ’¿ ç£ç›˜")
    print(f"   ä½¿ç”¨: {get_size(info['disk_used'])} / {get_size(info['disk_total'])}")
    filled = int(bar_width * info['disk_percent'] / 100)
    bar = 'â–ˆ' * filled + 'â–‘' * (bar_width - filled)
    print(f"   [{bar}] {info['disk_percent']:.1f}%")
    
    # ç½‘ç»œ
    print(f"\\nğŸŒ ç½‘ç»œ")
    print(f"   æ€»å‘é€: {get_size(info['net_sent'])}")
    print(f"   æ€»æ¥æ”¶: {get_size(info['net_recv'])}")
    if prev_net:
        sent_speed = (info['net_sent'] - prev_net[0]) / CONFIG['interval']
        recv_speed = (info['net_recv'] - prev_net[1]) / CONFIG['interval']
        print(f"   å‘é€é€Ÿåº¦: {get_size(sent_speed)}/s")
        print(f"   æ¥æ”¶é€Ÿåº¦: {get_size(recv_speed)}/s")
    
    # è¿›ç¨‹
    if processes:
        print(f"\\nğŸ“‹ Top {len(processes)} è¿›ç¨‹")
        print(f"   {'PID':>7} {'CPU%':>6} {'MEM%':>6} åç§°")
        for proc in processes:
            print(f"   {proc['pid']:>7} {proc['cpu_percent'] or 0:>5.1f}% {proc['memory_percent'] or 0:>5.1f}% {proc['name'][:30]}")
    
    print("\\n" + "-" * 60)
    print("æŒ‰ Ctrl+C é€€å‡º")

def monitor(config):
    """æŒç»­ç›‘æ§"""
    start_time = time.time()
    prev_net = None
    
    try:
        while True:
            info = get_system_info()
            processes = get_top_processes(config['show_processes'])
            print_monitor(info, processes, prev_net)
            
            prev_net = (info['net_sent'], info['net_recv'])
            
            if config['duration'] > 0:
                if time.time() - start_time > config['duration']:
                    print("\\nç›‘æ§æ—¶é—´åˆ°ï¼Œé€€å‡º...")
                    break
            
            time.sleep(config['interval'])
            
    except KeyboardInterrupt:
        print("\\n\\nç›‘æ§å·²åœæ­¢")

# ==================== ä¸»ç¨‹åº ====================
if __name__ == "__main__":
    if not HAS_PSUTIL:
        exit(1)
    
    parser = argparse.ArgumentParser(description='ç³»ç»Ÿç›‘æ§å·¥å…·')
    parser.add_argument('-i', '--interval', type=int, help='åˆ·æ–°é—´éš”ï¼ˆç§’ï¼‰')
    parser.add_argument('-d', '--duration', type=int, help='ç›‘æ§æ—¶é•¿ï¼ˆç§’ï¼‰')
    parser.add_argument('-n', '--top', type=int, help='æ˜¾ç¤ºtop Nè¿›ç¨‹')
    
    args = parser.parse_args()
    
    if args.interval:
        CONFIG['interval'] = args.interval
    if args.duration:
        CONFIG['duration'] = args.duration
    if args.top:
        CONFIG['show_processes'] = args.top
    
    monitor(CONFIG)
`;
  }

  private getCustomTemplate(): string {
    return `#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è‡ªå®šä¹‰å·¥å…·è„šæœ¬
# ç”¨æˆ·éœ€æ±‚: 
# ç”Ÿæˆæ—¶é—´: 
"""

import os
import sys
import argparse

# ==================== é…ç½®åŒºåŸŸ ====================
CONFIG = {
    # åœ¨è¿™é‡Œæ·»åŠ é…ç½®é¡¹
}

# ==================== ä¸»è¦åŠŸèƒ½ ====================
def main():
    """ä¸»å‡½æ•°"""
    print("è‡ªå®šä¹‰å·¥å…·å·²å¯åŠ¨")
    print("-" * 50)
    
    # åœ¨è¿™é‡Œæ·»åŠ ä¸»è¦é€»è¾‘
    
    print("-" * 50)
    print("âœ… æ‰§è¡Œå®Œæˆ")

# ==================== ä¸»ç¨‹åº ====================
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='è‡ªå®šä¹‰å·¥å…·')
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°
    # parser.add_argument('-i', '--input', help='è¾“å…¥')
    
    args = parser.parse_args()
    main()
`;
  }
}
